import streamlit as st
from langchain_anthropic import ChatAnthropic
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents.base import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain_core.runnables import Runnable
from langchain.schema.output_parser import StrOutputParser
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.memory import ConversationSummaryMemory
from langchain.chains import ConversationChain
from typing import List, Tuple
import os
import csv
import time
import uuid

# Anthropic API í‚¤ ë¡œë“œ (Streamlit secrets ì‚¬ìš©)
time.sleep(1)
api_key = st.secrets["anthropic"]["API_KEY"]

# PDF ì¸ë±ìŠ¤ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ì™€ ë‹¤ë¥¸ í´ë˜ìŠ¤ë“¤ì€ ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ ìœ ì§€

class RAGSystem:
    def __init__(self, api_key: str):
        self.api_key = api_key
        
        # Claude 3.5 Haiku ëª¨ë¸ë¡œ ë³€ê²½
        self.llm = ChatAnthropic(
            model="claude-3-5-haiku-20240307", 
            anthropic_api_key=self.api_key,
            temperature=0.1,  # ì¼ê´€ëœ ì‘ë‹µì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„ ì„¤ì •
            max_tokens=1000
        )
        
        # ëŒ€í™” ê¸°ì–µ ëª¨ë“ˆ ì´ˆê¸°í™”
        self.memory = ConversationSummaryMemory(llm=self.llm)
        self.conversation_chain = ConversationChain(
            llm=self.llm, 
            memory=self.memory, 
            verbose=True
        )

    def get_rag_chain(self) -> Runnable:
        template = """
        ğŸ“š ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë§ì¶¤í˜• ì‘ë‹µ ê°€ì´ë“œë¼ì¸:

        1. **ëŒ€í™” ì „ì²´ ë§¥ë½ ê³ ë ¤**: ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì² ì €íˆ ë¶„ì„í•˜ê³  ì—°ê²°í•©ë‹ˆë‹¤.
        2. **ì¼ê´€ì„± ìœ ì§€**: ì´ì „ ë‹µë³€ê³¼ ëª¨ìˆœë˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•©ë‹ˆë‹¤.
        3. ë‹µë³€ì€ ìµœëŒ€ 4ë¬¸ì¥, ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•©ë‹ˆë‹¤.
        4. ì¤‘ìš” ë‚´ìš©ì€ í•µì‹¬ë§Œ ìš”ì•½í•´ì„œ ì „ë‹¬í•©ë‹ˆë‹¤.
        5. **ìƒí™©ë³„ ëŒ€ì‘**:
           - ë°˜ë³µ ì§ˆë¬¸: ìƒˆë¡œìš´ ê´€ì  ë˜ëŠ” ì¶”ê°€ ì •ë³´ ì œê³µ
           - ëª¨í˜¸í•œ ì§ˆë¬¸: êµ¬ì²´ì  ë§¥ë½ í™•ì¸ í›„ ë‹µë³€
           - ì—°ì† ì§ˆë¬¸: ì´ì „ ëŒ€í™” íë¦„ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ê¸°

        ëŒ€í™” ì´ë ¥: {history}
        PDF ì»¨í…ìŠ¤íŠ¸: {context}
        í˜„ì¬ ì§ˆë¬¸: {question}

        ì‘ë‹µ ì‘ì„±:
        """
        prompt = PromptTemplate.from_template(template)
        
        # Claude 3.5 Haiku ëª¨ë¸ ì‚¬ìš©
        model = ChatAnthropic(
            model="claude-3-5-haiku-20240307", 
            anthropic_api_key=self.api_key,
            temperature=0.1,
            max_tokens=1000
        )
        return prompt | model | StrOutputParser()

    def process_question(self, question: str) -> str:
        # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (OpenAI ì„ë² ë”© ê·¸ëŒ€ë¡œ ì‚¬ìš©)
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=os.environ.get("OPENAI_API_KEY"))
        vector_db = FAISS.load_local("faiss_index_internal", embeddings, allow_dangerous_deserialization=True)
        retriever = vector_db.as_retriever(search_kwargs={"k": 10})
        docs = retriever.invoke(question)
        
        # ëŒ€í™” ê¸°ë¡ ìš”ì•½ ê°€ì ¸ì˜¤ê¸°
        conversation_history = self.memory.chat_memory.messages

        # RAG ì²´ì¸ ìƒì„±
        chain = self.get_rag_chain()

        # ëŒ€í™” ê¸°ë¡ê³¼ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ì—¬ ë‹µë³€ ìƒì„±
        answer = chain.invoke({
            "question": question, 
            "context": docs, 
            "history": conversation_history
        })

        # ëŒ€í™” ì²´ì¸ì— ëŒ€í™” ì¶”ê°€
        self.conversation_chain.predict(input=question)

        return answer

# ë©”ì¸ í•¨ìˆ˜
def main():
    st.set_page_config(page_title="ë””ì§€í„¸ê²½ì˜ì „ê³µ ì±—ë´‡", layout="wide")

    st.title("ğŸ“ ë””ì§€í„¸ê²½ì˜ì „ê³µ ì±—ë´‡")
    st.caption("ì—¬ëŸ¬ë¶„ì˜ í•™ê³¼ ê´€ë ¨ ê¶ê¸ˆì¦ì„ ë¹ ë¥´ê²Œ í•´ê²°í•´ë“œë¦½ë‹ˆë‹¤!")

    if st.button("ğŸ“¥ ì±„íŒ… ì‹œì‘ !"):
        generate_faiss_index()

    if "messages" not in st.session_state:
        st.session_state.messages = []

    left_col, mid_col, right_col = st.columns([1, 2.5, 1.2])

    with left_col:
        st.subheader("ğŸ“š ì‚¬ìš© ê°€ì´ë“œ")
        st.markdown("""
        - ì±„íŒ… ì‹œì‘! ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.<br>
        - ê¶ê¸ˆí•œ ì ì— ëŒ€í•´ì„œ ë¬¼ì–´ë³´ì„¸ìš” !.<br>
        - ì¶”ê°€ ë¬¸ì˜ëŠ” ë””ì§€í„¸ê²½ì˜ì „ê³µ í™ˆí˜ì´ì§€ë‚˜ í•™ê³¼ ì‚¬ë¬´ì‹¤(044-860-1560)ë¡œ ë¬¸ì˜í•´ ì£¼ì„¸ìš”.
        """, unsafe_allow_html=True)

    with mid_col:
        for msg in st.session_state.messages:
            if msg["role"] == "user":
                st.markdown(f"""
                <div style='background-color: #731034; padding: 10px; border-radius: 20px; margin-bottom: 10px; color: white; max-width: 70%; box-shadow: 0px 2px 5px rgba(0,0,0,0.1);'>
                ğŸ’¬ <b>ì§ˆë¬¸:</b> {msg["content"]}
                </div>""", unsafe_allow_html=True)
            else:
                st.markdown(f"""
                <div style='background-color: #f8f8f8; padding: 10px; border-radius: 20px; margin-bottom: 10px; margin-left: auto; box-shadow: 0px 2px 5px rgba(0,0,0,0.1); max-width: 70%;'>
                ğŸ¤– <b>ë‹µë³€:</b> {msg["content"]}
                </div>""", unsafe_allow_html=True)

        prompt = st.chat_input("ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")

        if prompt:
            st.session_state.messages.append({"role": "user", "content": prompt})
            rag = RAGSystem(api_key)

            previous_qa = None
            if len(st.session_state.messages) >= 2:
                prev_question = st.session_state.messages[-2]["content"]
                prev_answer = st.session_state.messages[-1]["content"]
                previous_qa = (prev_question, prev_answer)

            with st.spinner("ì§ˆë¬¸ì„ ì´í•´í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš” ğŸ˜Š"):
                answer = rag.process_question(prompt, previous_qa)
            st.session_state.messages.append({"role": "assistant", "content": answer})
            st.rerun()

    with right_col:
        st.subheader("ğŸ“¢ ê°œë°œìì—ê²Œ ì˜ê²¬ ë³´ë‚´ê¸°")
        feedback_input = st.text_area("ì±—ë´‡ì— ëŒ€í•œ ê°œì„  ì˜ê²¬ì´ë‚˜ í•˜ê³  ì‹¶ì€ ë§ì„ ë‚¨ê²¨ì£¼ì„¸ìš”!")
        if st.button("í”¼ë“œë°± ì œì¶œ"):
            if feedback_input.strip() != "":
                with open("feedback_log.csv", mode="a", encoding="utf-8-sig", newline="") as file:
                    writer = csv.writer(file)
                    writer.writerow([time.strftime('%Y-%m-%d %H:%M:%S'), feedback_input])
                st.success("ì†Œì¤‘í•œ ì˜ê²¬ ê°ì‚¬í•©ë‹ˆë‹¤!")
                st.rerun()
            else:
                st.warning("í”¼ë“œë°± ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")

        st.subheader("ğŸ“ ìµœê·¼ ì§ˆë¬¸ íˆìŠ¤í† ë¦¬")
        for i, q in enumerate([m["content"] for m in st.session_state.messages if m["role"] == "user"][-5:], 1):
            st.markdown(f"{i}. {q}")

if __name__ == "__main__":
    main()
